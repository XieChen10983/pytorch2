1. pytorch中网络的运算、损失的运算等需要用到梯度的运算，可以用autograd来自动获得。
    (1) pytorch中自带的autograd操作-->这些操作可以在nn中直接import：
        A. 激活函数类：
            ReLU、LeakyReLU、LogSigmoid、Sigmoid、Softmax等
        B. batchnorm类：
            BatchNorm2d、BatchNorm1d、BatchNorm3d、SyncBatchNorm等
        C. dropout类：
            Dropout等
        D. pooling类：
            MaxPool2d等
        E. 卷积计算类：
            Conv2d、Conv1d、Conv3d、ConvTranspose2d、ConvTranspose1d、ConvTranspose3d等
        F. 线形计算类：
            Linear、BiLinear
        G. 损失计算类：
            BCELoss、BCEWithLogitsLoss、L1Loss、MSELoss、HingeEmbeddingLoss等

    (2) 有些操作pytorch本身没有自带或者自己想改写pytorch中的autograd操作：
        A. 需要继承torch.autograd.Function进行扩展
        B. 需要完善其中的forward和backward函数，具体的模板如下：
            class My_Function(Function):
                def forward(self, inputs, parameters):
                    self.saved_for_backward = [inputs, parameters]
                    # output = [对输入和参数进行的操作，其实就是前向运算的函数表达式]
                    return output

                def backward(self, grad_output):
                    inputs, parameters = self.saved_tensors # 或者是self.saved_variables
                    # grad_inputs = [求函数forward(input)关于 parameters 的导数，其实就是反向运算的导数表达式] * grad_output
                    return grad_input
        C. 调用自己的autograd操作时，使用.apply
            my_func = My_Function.apply
迁移学习的两种主要形式如下：
    1. 对ConvNet进行微调：不随机初始化，而是使用一个训练好的网络来初始化，之后像往常一样训练。
    2. 将ConvNet作为固定的特征提取器：除了最后的全连接层，我们冻结所有网络的权重。最后一个完全的连接层被一个
        具有随机权值的新的连接层代替，只有这个层被训练。

两种形式的应用场景：
    1. 数据较少，并且数据与原训练数据相似度较高：
        不应使用fine-tuning，因为很可能会过拟合。
    2. 数据较大，并且与原训练数据较为相似。
        可以使用fine-tuning，有较大的信心不过拟合。
    3. 数据较小，并且与原数据非常不同
        因为数据较小，我们最好只训练一个线性的分类器；由于与原数据差别较大，最好不要从网络顶部训练分类器，
        而是从网络较早的某个位置进行分类器的训练。因为越往后，网络将包含越多前面数据集的信息。
    4. 数据较大并且和原数据非常不同
        我们可以完全从头训练一个网络。但是实际上我们仍然采用一个训练好的网络来初始化，然后使用数据对整个
        网络进行fine-tine。

迁移学习的一些注意事项：
    1. 尽量不要移除卷积层
    2. 学习率尽量比从头训练时小一些，因为我们认为pretrained的模型参数是较好的，因此不需要变化这么快。